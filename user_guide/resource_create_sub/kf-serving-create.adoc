= Creating KFServing
:toc:
:toc-title:

The following describes how to create KFServing resources.

== YAML Editor

Create new KFServing by using the YAML editor.

. Click *[KFServing]* in the <<../console_menu_sub/ai-dev#img-kf-serving-main,KFServing>> main screen.
. Enter configuration data about KFServing to create in the YAML editor opened.
+
.Sample
[source,yaml]
----
apiVersion: serving.kubeflow.org/v1alpha2
kind: InferenceService
metadata: <1>
  name: demo-inferenceservice <2>
  namespace: demo <3>
spec: <4>
  default: <5>
    predictor: <6>
      tensorflow: <7>
        resources: <8>
          limits: <9>
            cpu: 100m
            memory: 1Gi
          requests: <10>
            cpu: 100m
            memory: 1Gi
        runtimeVersion: 1.14.0 <11>
        storageUri: pvc://demo-pvc/saved_model <12>
----
+
<1> InferenceService metadata
<2> InferenceService name
<3> Name of namespace where the InferenceService is created
<4> InferenceService specification
<5> Basic Inference Server specification
<6> ML predictor specification
<7> ML framework used
<8> InferenceService resource amount
<9> Amount of resource limits
<10> Amount of resource requests
<11> TensorFlow version
<12> Path to a ML model used in Inference Server

. To create the resource with the data you entered, click *[Create]*.